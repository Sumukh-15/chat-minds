# ðŸ¤– ChatMinds â€“ Your Personal AI Chat Assistant

Welcome to **ChatMinds**, an AI-powered chatbot built using **Streamlit**, **LangChain**, and **Ollama** that runs entirely **locally**. No OpenAI API keys or internet required â€” just your own CPU and curiosity!

![Chat UI Preview](image.png)

---

## ðŸš€ Features

- ðŸ” **Secure login/logout** with `streamlit-authenticator`
- ðŸ§  **Context-aware chat memory** (remembers past interactions)
- âš¡ **Runs fully offline** using local Ollama models like `mistral`, `llama2`, etc.
- ðŸ§© **Modular structure** for easy customization
- ðŸ§¼ **Clear Chat** option to reset conversations
- ðŸŽ¨ Clean UI with sidebar navigation

---

## ðŸ› ï¸ Tech Stack

- [Streamlit](https://streamlit.io/) â€“ Web UI
- [LangChain](https://www.langchain.com/) â€“ Prompt & memory management
- [Ollama](https://ollama.com/) â€“ Local AI model runner (supports `mistral`, `llama2`, etc.)
- [streamlit-authenticator](https://github.com/mkhorasani/Streamlit-Authenticator) â€“ Login/logout system

---

## ðŸ“ Folder Structure

chat-minds/
â”œâ”€â”€ main.py
â”œâ”€â”€ config_sample.yaml
â”œâ”€â”€ hash_sample_pass.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt


---

## ðŸ”§ Setup Instructions

## âœ… Install Ollama and model

Install [Ollama](https://ollama.com/download) and run:

```bash
~ollama run mistral

Or use other models:
~ollama pull llama2
```
---

## âœ… Create virtual environment
```bash
~python -m venv venv
~source venv/bin/activate  # On Windows: venv\Scripts\activate
```

## âœ… 3. Install dependencies
```bash
~pip install -r requirements.txt
```
=>requirements.txt/
    streamlit
    streamlit-authenticator
    pyyaml
    langchain
    langchain-community
    langchain-core
    langchain-ollama
    ollama
    bcrypt

## âœ… 4. Prepare configuration
Rename config_sample.yaml to config.yaml and update usernames and hashed passwords.

To hash a new password:
```bash
~python hash_sample_pass.py
```

## â–¶ï¸ Run the App
```bash
~streamlit run main.py
```
=>Then open http://localhost:8501 in your browser.

## ðŸ§  Supported Models
You can easily swap models by updating:

llm = OllamaLLM(model="mistral")
Some options:
*mistral
*llama2
*gemma
*codellama
*phi3

## ðŸ“œ License
=>This project is licensed under the MIT License.

## ðŸŒŸ Acknowledgments
=>Streamlit
=>LangChain
=>Ollama
=>Streamlit Authenticator
